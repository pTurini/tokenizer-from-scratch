{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "Tokenization is the process of taking text input and breaking them down into smaller inputs: characters, words, subwords.\n",
    "These are then mapped to IDs, and form the model's vocabulary, the set of all possible tokens.\n",
    "\n",
    "This notebook aims to implement a tokenizer. After this, I plan to develop an embedding model, which takes these tokens and maps them to vectors in a semantic space, to be fed to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few important observations: \n",
    "* the same concept might have 2+ different token mapping depending on the context: lower case, upper case, in the beginning of the sentence, at the end of the sentence, etc.\n",
    "\n",
    "* non-English languages end up having shorter tokens because there is less data from them ---> more tokens/more chunks for the same concepts.  This is particularly bad for context windows, in self-attention since there are more tokens in a simple sentence.\n",
    "\n",
    "* for coding, indentation causes ineficiencies stemming from wasteful use of the context window using one token per space.\n",
    "\n",
    "A good tokenizer can decrease the number of tokens per sentence while not increasing the vocabulary too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 12354, 129300, 38738)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the Unicode code points:\n",
    "ord('a'), ord('„ÅÇ'), ord('ü§î'), ord('Èùí')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87,\n",
       " 104,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 115,\n",
       " 116,\n",
       " 97,\n",
       " 110,\n",
       " 100,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 119,\n",
       " 97,\n",
       " 121,\n",
       " 32,\n",
       " 98,\n",
       " 101,\n",
       " 99,\n",
       " 111,\n",
       " 109,\n",
       " 101,\n",
       " 115,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 32,\n",
       " 119,\n",
       " 97,\n",
       " 121,\n",
       " 46]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encodings translate the unicode point to a byte string -> UTF-8 encodes it to 8 bytes. \n",
    "list(\"What stands in the way becomes the way.\".encode('utf-8'))\n",
    "# Using UTF is inadequate, because the words are mapped to many bytes, so the context used would be too large, \n",
    "# and the byte-wise prediction too myopic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE: Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE finds the pair of tokens that occur most frequently, iteratively, appending this token to the vocab.\n",
    "\n",
    "aaabdaaabac\n",
    "\n",
    "aa is mapped into Z.\n",
    "Then, the sentence becomes ZabdZabac. ab occurs most frequently, so it turns into Y:\n",
    "\n",
    "ZYdZYac\n",
    "\n",
    "ZY appears most frequently, mapping it into XÔºö\n",
    "\n",
    "XdXac\n",
    "\n",
    "The most frequent pair only appears once, so the algorithm terminates.\n",
    "* X=ZY\n",
    "* Y=ab\n",
    "* Z=aa\n",
    "\n",
    "The resulting sequence is compressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 32, 116, 101, 108, 108, 32, 121, 111, 117, 58, 32, 111, 110, 101, 32, 109, 117, 115, 116, 32, 115, 116, 105, 108, 108, 32, 104, 97, 118, 101, 32, 99, 104, 97, 111, 115, 32, 105, 110, 32, 111, 110, 101, 44, 32, 116, 111, 32, 103, 105, 118, 101, 32, 98, 105, 114, 116, 104, 32, 116, 111, 32, 97, 32, 100, 97, 110, 99, 105, 110, 103, 32, 115, 116, 97, 114, 46, 32, 73, 32, 116, 101, 108, 108, 32, 121, 111, 117, 58, 32, 121, 101, 32, 104, 97, 118, 101, 32, 115, 116, 105, 108, 108, 32, 99, 104, 97, 111, 115, 32, 105, 110, 32, 121, 111, 117, 46, 10, 65, 108, 97, 115, 33, 32, 84, 104, 101, 114, 101, 32, 99, 111, 109, 101, 116, 104, 32, 116, 104, 101, 32, 116, 105, 109, 101, 32, 119, 104, 101, 110, 32, 109, 97, 110, 32, 119, 105, 108, 108, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 103, 105, 118, 101, 32, 98, 105, 114, 116, 104, 32, 116, 111, 32, 97, 110, 121, 32, 115, 116, 97, 114, 46, 32, 65, 108, 97, 115, 33, 32, 84, 104, 101, 114, 101, 32, 99, 111, 109, 101, 116, 104, 32, 116, 104, 101, 32, 116, 105, 109, 101, 32, 111, 102, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 100, 101, 115, 112, 105, 99, 97, 98, 108, 101, 32, 109, 97, 110, 44, 32, 119, 104, 111, 32, 99, 97, 110, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 100, 101, 115, 112, 105, 115, 101, 32, 104, 105, 109, 115, 101, 108, 102, 46, 10, 76, 111, 33, 32, 73, 32, 115, 104, 111, 119, 32, 121, 111, 117, 32, 84, 72, 69, 32, 76, 65, 83, 84, 32, 77, 65, 78, 46, 10, 226, 128, 156, 87, 104, 97, 116, 32, 105, 115, 32, 108, 111, 118, 101, 63, 32, 87, 104, 97, 116, 32, 105, 115, 32, 99, 114, 101, 97, 116, 105, 111, 110, 63, 32, 87, 104, 97, 116, 32, 105, 115, 32, 108, 111, 110, 103, 105, 110, 103, 63, 32, 87, 104, 97, 116, 32, 105, 115, 32, 97, 32, 115, 116, 97, 114, 63, 226, 128, 157, 226, 128, 148, 115, 111, 32, 97, 115, 107, 101, 116, 104, 32, 116, 104, 101, 32, 108, 97, 115, 116, 32, 109, 97, 110, 32, 97, 110, 100, 32, 98, 108, 105, 110, 107, 101, 116, 104, 46, 240, 159, 148, 165, 32]\n"
     ]
    }
   ],
   "source": [
    "# Implementation:\n",
    "text= \"\"\"I tell you: one must still have chaos in one, to give birth to a dancing star. I tell you: ye have still chaos in you.\n",
    "Alas! There cometh the time when man will no longer give birth to any star. Alas! There cometh the time of the most despicable man, who can no longer despise himself.\n",
    "Lo! I show you THE LAST MAN.\n",
    "‚ÄúWhat is love? What is creation? What is longing? What is a star?‚Äù‚Äîso asketh the last man and blinketh.üî• \"\"\"\n",
    "tokens = text.encode(\"utf-8\") # get the bytes\n",
    "tokens = list(map(int,tokens)) # convert them into a list of 0..255\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 429)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), len(tokens) # some characters are mapped into more than one byte, like the emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, (101, 32)), (11, (32, 116)), (10, (116, 104)), (8, (115, 116)), (8, (104, 97)), (7, (116, 32)), (7, (111, 32)), (7, (104, 101)), (7, (97, 110)), (6, (115, 32)), (6, (111, 110)), (6, (110, 32)), (6, (32, 115)), (6, (32, 105)), (6, (32, 99)), (5, (118, 101)), (5, (116, 105)), (5, (110, 103)), (5, (108, 108)), (5, (108, 32)), (5, (105, 115)), (5, (105, 110)), (5, (104, 32)), (5, (97, 116)), (5, (32, 121)), (5, (32, 109)), (5, (32, 108)), (5, (32, 97)), (4, (121, 111)), (4, (111, 117)), (4, (109, 101)), (4, (108, 111)), (4, (101, 116)), (4, (101, 114)), (4, (97, 115)), (4, (87, 104)), (3, (226, 128)), (3, (116, 111)), (3, (116, 97)), (3, (114, 101)), (3, (111, 115)), (3, (109, 97)), (3, (108, 97)), (3, (105, 109)), (3, (105, 108)), (3, (103, 105)), (3, (101, 108)), (3, (97, 114)), (3, (73, 32)), (3, (63, 32)), (3, (46, 10)), (3, (33, 32)), (3, (32, 119)), (3, (32, 111)), (3, (32, 104)), (3, (32, 100)), (3, (32, 98)), (3, (32, 87)), (3, (32, 84)), (2, (119, 104)), (2, (117, 58)), (2, (116, 101)), (2, (115, 112)), (2, (115, 101)), (2, (115, 33)), (2, (114, 116)), (2, (114, 46)), (2, (114, 32)), (2, (112, 105)), (2, (111, 109)), (2, (110, 111)), (2, (110, 101)), (2, (107, 101)), (2, (105, 118)), (2, (105, 114)), (2, (104, 111)), (2, (103, 101)), (2, (101, 115)), (2, (100, 101)), (2, (99, 111)), (2, (99, 104)), (2, (99, 97)), (2, (98, 108)), (2, (98, 105)), (2, (97, 118)), (2, (97, 111)), (2, (97, 32)), (2, (84, 104)), (2, (65, 108)), (2, (58, 32)), (2, (46, 32)), (2, (44, 32)), (2, (32, 110)), (2, (32, 103)), (2, (32, 73)), (1, (240, 159)), (1, (165, 32)), (1, (159, 148)), (1, (157, 226)), (1, (156, 87)), (1, (148, 165)), (1, (148, 115)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (121, 101)), (1, (121, 32)), (1, (119, 105)), (1, (119, 32)), (1, (117, 115)), (1, (117, 46)), (1, (117, 32)), (1, (115, 111)), (1, (115, 107)), (1, (115, 104)), (1, (114, 63)), (1, (111, 119)), (1, (111, 118)), (1, (111, 102)), (1, (111, 33)), (1, (110, 121)), (1, (110, 107)), (1, (110, 100)), (1, (110, 99)), (1, (110, 63)), (1, (110, 44)), (1, (109, 117)), (1, (109, 115)), (1, (109, 111)), (1, (108, 105)), (1, (108, 102)), (1, (108, 101)), (1, (105, 111)), (1, (105, 99)), (1, (104, 105)), (1, (104, 46)), (1, (103, 63)), (1, (103, 32)), (1, (102, 46)), (1, (102, 32)), (1, (101, 110)), (1, (101, 97)), (1, (101, 63)), (1, (101, 44)), (1, (100, 97)), (1, (100, 32)), (1, (99, 114)), (1, (99, 105)), (1, (97, 98)), (1, (84, 72)), (1, (84, 32)), (1, (83, 84)), (1, (78, 46)), (1, (77, 65)), (1, (76, 111)), (1, (76, 65)), (1, (72, 69)), (1, (69, 32)), (1, (65, 83)), (1, (65, 78)), (1, (63, 226)), (1, (46, 240)), (1, (32, 77)), (1, (32, 76)), (1, (32, 65)), (1, (10, 226)), (1, (10, 76)), (1, (10, 65))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats (text):\n",
    "    counts = {}\n",
    "    for pair in zip(text,text[1:]):\n",
    "        counts[pair] = counts.get(pair,0) +1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32) # the most common pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating new tokens, iterating\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 120, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids =[]\n",
    "    i = 0\n",
    "    while i <len(ids):\n",
    "        if i< len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i=i+2\n",
    "        else: \n",
    "            newids.append(ids[i])\n",
    "            i= i +1\n",
    "    return newids\n",
    "\n",
    "print(merge([1,2,2,4,5,6],(2,4),120)) # replaces (2,4) with 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 32, 116, 101, 108, 108, 32, 121, 111, 117, 58, 32, 111, 110, 256, 109, 117, 115, 116, 32, 115, 116, 105, 108, 108, 32, 104, 97, 118, 256, 99, 104, 97, 111, 115, 32, 105, 110, 32, 111, 110, 101, 44, 32, 116, 111, 32, 103, 105, 118, 256, 98, 105, 114, 116, 104, 32, 116, 111, 32, 97, 32, 100, 97, 110, 99, 105, 110, 103, 32, 115, 116, 97, 114, 46, 32, 73, 32, 116, 101, 108, 108, 32, 121, 111, 117, 58, 32, 121, 256, 104, 97, 118, 256, 115, 116, 105, 108, 108, 32, 99, 104, 97, 111, 115, 32, 105, 110, 32, 121, 111, 117, 46, 10, 65, 108, 97, 115, 33, 32, 84, 104, 101, 114, 256, 99, 111, 109, 101, 116, 104, 32, 116, 104, 256, 116, 105, 109, 256, 119, 104, 101, 110, 32, 109, 97, 110, 32, 119, 105, 108, 108, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 103, 105, 118, 256, 98, 105, 114, 116, 104, 32, 116, 111, 32, 97, 110, 121, 32, 115, 116, 97, 114, 46, 32, 65, 108, 97, 115, 33, 32, 84, 104, 101, 114, 256, 99, 111, 109, 101, 116, 104, 32, 116, 104, 256, 116, 105, 109, 256, 111, 102, 32, 116, 104, 256, 109, 111, 115, 116, 32, 100, 101, 115, 112, 105, 99, 97, 98, 108, 256, 109, 97, 110, 44, 32, 119, 104, 111, 32, 99, 97, 110, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 100, 101, 115, 112, 105, 115, 256, 104, 105, 109, 115, 101, 108, 102, 46, 10, 76, 111, 33, 32, 73, 32, 115, 104, 111, 119, 32, 121, 111, 117, 32, 84, 72, 69, 32, 76, 65, 83, 84, 32, 77, 65, 78, 46, 10, 226, 128, 156, 87, 104, 97, 116, 32, 105, 115, 32, 108, 111, 118, 101, 63, 32, 87, 104, 97, 116, 32, 105, 115, 32, 99, 114, 101, 97, 116, 105, 111, 110, 63, 32, 87, 104, 97, 116, 32, 105, 115, 32, 108, 111, 110, 103, 105, 110, 103, 63, 32, 87, 104, 97, 116, 32, 105, 115, 32, 97, 32, 115, 116, 97, 114, 63, 226, 128, 157, 226, 128, 148, 115, 111, 32, 97, 115, 107, 101, 116, 104, 32, 116, 104, 256, 108, 97, 115, 116, 32, 109, 97, 110, 32, 97, 110, 100, 32, 98, 108, 105, 110, 107, 101, 116, 104, 46, 240, 159, 148, 165, 32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(429, 413)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "len(tokens), len(tokens2) # reduced size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
